{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection (optional) #\n",
    "\n",
    "Now that the model is trained, we can take a look at outlier detection.  \n",
    "In this notebook, three different methods to identify outliers are presented, i.e. data-driven choices for distinguishing known from unknown (class not in training data) spectra. \n",
    "\n",
    "For spectra of N different classes, each of these classes is selected as outlier for one outlier training iteration.  \n",
    "In each iteration, the model is trained with N-1 classes (known spectra) and then tested with 1 class (unknown spectra).  \n",
    "With these results, the best, data-driven discrimination between known and unknown data can be found.\n",
    "\n",
    "This notebook only serves for exploratory purposes; you need to manually implement/update the final outlier criterion in your measurement pipeline. \n",
    "\n",
    "The python library used for model training and testing is `tools_outlier.py` and all visualizations are specified in `plotting.py`.  \n",
    "As we did before, we set the main parameters for model training and save them as global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.tools_outlier import *\n",
    "from tools.plotting import *\n",
    "\n",
    "# Set global variables\n",
    "dets_tr = [\"simulated+bg\"]  # only simulated spectra + backgrounds (default)\n",
    "GlobalVariables.dets_tr = dets_tr\n",
    "\n",
    "min_channel_tr = 7\n",
    "GlobalVariables.min_channel_tr = min_channel_tr\n",
    "\n",
    "min_scores_norm = 0.1\n",
    "GlobalVariables.min_scores_norm = min_scores_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data, all_isotopes = load_spectral_data(dir_numpy_ready, dets_tr)\n",
    "all_isotopes = [\n",
    "    x[0] for x in all_isotopes\n",
    "]  # only single-label spectra allowed for model training\n",
    "data = remove_empty_or_negative_spectra(data)\n",
    "\n",
    "# We need to identify the isotope with the fewest data (for class balance later on)\n",
    "list_counts = count_spectra_per_isotope(data)\n",
    "n_cut = min(list_counts)  # find minimum number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now choose the isotopes that should \"play\" outlier\n",
    "# Then, we loop over these isotopes, train the model based on the known isotopes and output the relevant data to best discriminate known from unknown isotopes\n",
    "\n",
    "# choose which isotopes are included in outlier analysis\n",
    "isotope_outliers = set_isotope_outliers(\n",
    "    all_isotopes\n",
    ")  # e.g. isotope_outliers = ['Am241', 'Co60', 'Cs137'] (without background)\n",
    "\n",
    "if len(isotope_outliers) == 0:\n",
    "    print(\"Error: No outlier isotopes were provided\")\n",
    "    exit()\n",
    "\n",
    "for i, isotope_outlier in enumerate(isotope_outliers):\n",
    "    # main function call\n",
    "    xi_known, xi_unknown = simulate_outlier(isotope_outlier)\n",
    "    # xi_known contains features of known isotopes (that were used in training)\n",
    "    # xi_unknown contains features of unknown isotopes (that were not used in training)\n",
    "\n",
    "    if i > 0:  # list is already defined, append to array\n",
    "        x_all_known = np.append(x_all_known, np.array(xi_known), axis=0)\n",
    "\n",
    "        # take n_cut random entries\n",
    "        x_all_unknown = np.append(\n",
    "            x_all_unknown,\n",
    "            np.array(xi_unknown)[\n",
    "                np.random.choice(np.arange(len(xi_unknown)), size=n_cut, replace=False),\n",
    "                :,\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "    else:  # first loop: set up array\n",
    "        x_all_known = np.array(xi_known)\n",
    "        x_all_unknown = np.array(xi_unknown)\n",
    "\n",
    "# The output \"explained variance ratio\" measures the quality of the model training:\n",
    "# Based on the training data (first number) this value should be close to 100%\n",
    "# Based on the unknown data (\"outlier\") this number should be <100% (usually around 10%). It can also be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Set an imbalance factor between known and unknown data ####\n",
    "\n",
    "Since it is not equally likely to encounter known and unknown spectra, you can set a factor of imbalance below.  \n",
    "This will adjust the sizes of the datasets for the further analyses.  \n",
    "In our example, we set `factor_imbalance = 10` for illustrative purposes.\n",
    "The imbalance factor quantifies how much more known than unknown data we have in the training.\n",
    "On average, the final model will hence predict that every `factor_imbalance` spectra is an outlier.\n",
    "A higher value will make the model more sensitive towards outliers, but also creates more false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_imbalance = 10  # here: factor 10 between known and unknown spectra\n",
    "x_all_known = x_all_known[: factor_imbalance * x_all_unknown.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the label column to identify known /\n",
    "known_labels = np.zeros((x_all_known.shape[0], 1))  # 0 for all rows in x_all_known\n",
    "unknown_labels = np.ones((x_all_unknown.shape[0], 1))  # 1 for all rows in x_all_unknown\n",
    "\n",
    "# Stack the numpy arrays vertically\n",
    "x_combined = np.vstack((x_all_known, x_all_unknown))\n",
    "\n",
    "# Stack the label column with the combined data\n",
    "labels_combined = np.vstack((known_labels, unknown_labels))\n",
    "\n",
    "# Get the number of columns in x_combined\n",
    "num_scores = (\n",
    "    x_combined.shape[1] - 5\n",
    ")  # Number of columns in x_combined minus manual values\n",
    "\n",
    "# Dynamically generate the \"scoreX\" column names based on the number of columns in x_combined\n",
    "score_column_names = [\n",
    "    f\"score{i + 1}\" for i in range(num_scores)\n",
    "]  # Adjust based on how many additional columns are fixed\n",
    "\n",
    "# Add any fixed column names after the dynamically generated score columns\n",
    "other_column_names = [\n",
    "    \"scores_mean\",\n",
    "    \"scores_median\",\n",
    "    \"expl.var.\",\n",
    "    \"cos.sim.\",\n",
    "    \"scores_norm_abs\",\n",
    "    \"label\",\n",
    "]\n",
    "\n",
    "# Combine dynamically generated score columns with other fixed columns\n",
    "column_names = score_column_names + other_column_names\n",
    "\n",
    "check_column_names_match(x_combined, labels_combined, column_names)\n",
    "\n",
    "# Combine the arrays into a pandas DataFrame\n",
    "df_combined = pd.DataFrame(\n",
    "    np.hstack((x_combined, labels_combined)), columns=column_names\n",
    ")\n",
    "\n",
    "# Shuffle data\n",
    "df_combined = df_combined.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(\n",
    "    \"The following information is available and used for the model training to discriminate known from unknown spectra.\"\n",
    ")\n",
    "print(\"We show the first fews lines as an example:\")\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tot = len(df_combined)\n",
    "f_train = 0.8  # fraction of training data\n",
    "n_train = int(f_train * n_tot)\n",
    "\n",
    "# Split the data into a training and a testing set (here, Ntrain is the number of lines for training)\n",
    "train_features = np.array(df_combined.iloc[:n_train, :-1])\n",
    "train_targets = np.array(df_combined.iloc[:n_train, -1])\n",
    "\n",
    "test_features = np.array(df_combined.iloc[n_train:, :-1])\n",
    "test_targets = np.array(df_combined.iloc[n_train:, -1])\n",
    "\n",
    "test_cos_sim = np.array(\n",
    "    df_combined[\"cos.sim.\"]\n",
    ")  # because this will be the important quantity later\n",
    "all_targets = np.array(df_combined.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Finding the best feature for outlier identification ##\n",
    "Let's train a decision tree model to predict the best discrimination between known and unknown spectra.  \n",
    "Decision trees are very transparent machine learning models, where the data set is split based on a set of consecutive rules.  \n",
    "How to read the decision tree plot (see below): \n",
    "- The first line of reach cells show the condition based on which a split is performed.\n",
    "- If the condition is TRUE, data goes to the left side. If it is FALSE, data goes to the right side.\n",
    "- values = [known spectra, outlier spectra] shows the number of spectra that are still in a node.\n",
    "- The objective is to get these values as \"pure\" as possible. \n",
    "- The pureness of the sample is given be the (information) entropy. \n",
    "- The construction algorithm of a decision tree tries to minimize the entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "tree_depth = 2\n",
    "tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=tree_depth).fit(\n",
    "    train_features, train_targets\n",
    ")\n",
    "\n",
    "# Predict the classes of new, unseen data\n",
    "prediction = tree.predict(test_features)\n",
    "\n",
    "# Check the accuracy\n",
    "accuracy = accuracy_score(test_targets, prediction)\n",
    "precision = precision_score(test_targets, prediction)\n",
    "recall = recall_score(test_targets, prediction)\n",
    "print(f\"{accuracy=:.3f}, {precision=:.3f}, {recall=:.3f}\")\n",
    "\n",
    "# plot tree\n",
    "feature_names = df_combined.columns[:-1]\n",
    "fig_tree = plt.figure(figsize=(12, 3.5))\n",
    "fig_tree.suptitle(\"Decision tree for outlier detection\")\n",
    "skl.tree.plot_tree(\n",
    "    tree,\n",
    "    max_depth=tree_depth,\n",
    "    feature_names=feature_names,\n",
    "    class_names=[\"known spectra\", \"outlier spectra\"],\n",
    "    proportion=True,\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "plot_outlier_confusion(test_targets, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Feature Importance ###\n",
    "\n",
    "We can analyze the importance of the features to decide which feature will be used to distinguish known from unknown features.  \n",
    "In our example, the cosine similarity is the most important feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = permutation_importance(\n",
    "    tree, train_features, train_targets, n_repeats=5\n",
    ")\n",
    "y = feature_importance[\"importances_mean\"]\n",
    "\n",
    "plot_feature_importance(feature_names, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Decision boundary for most important feature\n",
    "In the example dataset, the cosine similarity was identified as most important feature.  \n",
    "From here, a decision boundary can be derived to distinguish between known and unknown spectra in three different ways: \n",
    "\n",
    "\n",
    "#### a) Using the decision boundary from the decision tree:\n",
    "As a first option, we can use the decision tree visualized above.  \n",
    "The optimal decision boundary for the most important feature can be read from the condition of the first split.  \n",
    "In our example, the optimal threshold for the cosine similarity is 0.645. \n",
    "\n",
    "#### b) Fitting the decision boundary (logistic regression)\n",
    "Alternatively, the outlier score of a spectrum (1 for outliers and 0 for known spectra) can be plotted against the most important feature.   \n",
    "In our example, the plot shows that known spectra exhibit high cosine similarities while outliers tend to have low cosine similarities.  \n",
    "Next, a sigmoid function is fitted to the data to find the decision boundary. In our case, the optimal threshold is at x0 = 0.47.  \n",
    "You can implement the sigmoid and the fitted parameters in your measurement pipeline to predict the probability of a new spectrum to be an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data, x_fit, y_fit = fit_logistic_regression_for_outlier_feature(\n",
    "    df_combined, feature=\"cos.sim.\"\n",
    ")\n",
    "\n",
    "plot_fitted_sigmoid(x_data, y_data, x_fit, y_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Setting a manual decision boundary \n",
    "\n",
    "Alternatively, the decision boundary for the most important feature can be set manually to separate known and unknown spectra.  \n",
    "On this account, the accuracy, precision and recall for different thresholds between 0 and 1 are calculated and visualized below.  \n",
    "The plot can be read as follows: \n",
    "- **Accuracy** shows how often the outlier detection model is correct overall.  \n",
    "- **Precision** shows how often the outlier detection model is correct when predicting \"outlier\".  \n",
    "- **Recall** shows whether the outlier detection model can find all outliers in the data. \n",
    "- A higher threshold for the cosine similarity means that more spectra will be labelled as outliers.\n",
    "- For the extreme threshold of 0, no spectra are labelled as outlier, leading to 90% accuracy as all outlier spectra (10% of the data) are misclassified \n",
    "- Inversely, for an extreme threshold of 1, all spectra are labelled as outliers, leading to 10% accuracy as all known spectra (90% of the data) are misclassified \n",
    "- In our example, a reasonnable choice for the decision boundary would be around 0.5 - 0.7 (balance between accuracy, precision, and recall)\n",
    "\n",
    "Given this information, you can choose a threshold for outlier detection as a direct criterion and implement it in your measurement pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics\n",
    "stepsize = 0.002\n",
    "thresholds = np.arange(\n",
    "    0.0, 1.0 + stepsize, stepsize\n",
    ")  # list of thresholds from 0.5 to 1 in steps of 0.001\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for thresh in thresholds:  # iterate over thresholds\n",
    "    prediction = SimplePredict(\n",
    "        test_cos_sim, cut=thresh\n",
    "    )  # distinguish known & unknown data at threshold\n",
    "\n",
    "    # calculate metrics and store in lists\n",
    "    accuracies.append(accuracy_score(all_targets, prediction))\n",
    "    precisions.append(precision_score(all_targets, prediction))\n",
    "    recalls.append(recall_score(all_targets, prediction))\n",
    "\n",
    "# plot accuracy, precision and recall vs. thresholds\n",
    "plot_metrics_vs_threshold(thresholds, accuracies, precisions, recalls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
